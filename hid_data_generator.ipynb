{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# 创建一个日志记录器\n",
    "logger = logging.getLogger('visual_logger')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# 创建一个控制台处理程序，将日志显示在终端\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "\n",
    "# 创建日志格式器\n",
    "log_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "console_handler.setFormatter(log_formatter)\n",
    "\n",
    "# 将处理程序添加到记录器\n",
    "logger.addHandler(console_handler)\n",
    "# 假设数据集文件名为 'IMDB Dataset.csv'\n",
    "file_path = 'datas\\\\IMDB_Dataset.csv'\n",
    "\n",
    "# 使用pandas读取CSV文件\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    25000\n",
       "negative    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_hidden_state_path = \"E:\\\\github_project\\\\ChatGLM3\\\\THUDM\\\\hids\"\n",
    "target_hidden_state_path = \"./datas/IMDB_hids_no_extra_prompt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from IPython.display import display\n",
    "from Taotie.agent_use import ChatAgent\n",
    "# TaskDataGenerator可以向本地chatglm3-6b批量发送任务请求，并将任务推理过程中的相关数据（对话内容与隐藏层状态参数）保存整理\n",
    "class TaskDataGenerator():\n",
    "    def __init__(self, task_pattern, local_model = \"chatglm3-6b\"):\n",
    "        self.task_pattern = task_pattern\n",
    "        self.model_name = local_model\n",
    "        self.task_prompts = None\n",
    "        self.text_prompts = None\n",
    "        self.ori_hidden_state_path = ori_hidden_state_path\n",
    "        self.target_hidden_state_path = target_hidden_state_path\n",
    "        self.processing_text_index = 0\n",
    "        self.agent = ChatAgent(self.model_name)\n",
    "        logger.info('using local model: {self.model_name}')\n",
    "        if not os.path.exists(self.target_hidden_state_path):\n",
    "            os.mkdir(self.target_hidden_state_path)\n",
    "\n",
    "    def start_fetch(self, start_index, end_index):\n",
    "        total = end_index - start_index\n",
    "        for i in range(start_index, end_index):\n",
    "            self.processing_text_index = i\n",
    "            self.fetch_one_date()\n",
    "\n",
    "            progress_percentage = ((i - start_index + 1) / total) * 100\n",
    "            print(f\"\\rProgress: {self.processing_text_index - start_index}/{total} \\t {progress_percentage:.2f}%\", end=\"\")\n",
    "\n",
    "            # 可选：如果需要在一行中刷新输出，可以添加\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    def fetch_one_date(self):\n",
    "        self.check_ori_hidden_state_path_empty()\n",
    "        # 取出特定位置的内容来进行编码\n",
    "        self.agent.prompt_add(df.iloc[self.processing_text_index]['review'])\n",
    "        result = self.agent.prompt_post(maxtokens=1)\n",
    "        # 代理初始化\n",
    "        self.agent.messages = []\n",
    "        self.hidden_state_move()\n",
    "\n",
    "    def check_ori_hidden_state_path_empty(self):\n",
    "        # 检查self.ori_hidden_state_path文件夹中是否为空\n",
    "        return not os.listdir(self.ori_hidden_state_path)\n",
    "\n",
    "    def hidden_state_move(self):\n",
    "        # 将参数文件移入具体的index对应文件夹中\n",
    "        for file in os.listdir(self.ori_hidden_state_path):\n",
    "            os.rename(os.path.join(self.ori_hidden_state_path, file), os.path.join(self.target_hidden_state_path,f'{str(self.processing_text_index)}'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 20:38:36,147 - my_logger - INFO - api_base:openai\tllm version: chatglm3-6b\n",
      "2023-12-05 20:38:36,151 - visual_logger - INFO - using local model: {self.model_name}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 16124/16125 \t 100.00%"
     ]
    }
   ],
   "source": [
    "test_TaskDataGenerator = TaskDataGenerator(task_pattern=\"datas/translator\")\n",
    "# 根据 en_ch_ori_data_processed.txt 文件前1000行文本执行翻译任务\n",
    "# test_TaskDataGenerator.start_fetch(33875, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tmp = np.load(f\"{target_hidden_state_path}/0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(tmp.values())).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from IPython.display import clear_output\n",
    "\n",
    "file_path = 'datas\\\\IMDB_Dataset.csv'\n",
    "\n",
    "# 使用pandas读取CSV文件\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Splitting the DataFrame into training and test sets (80% training, 20% test)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['sentiment'], random_state=42)\n",
    "\n",
    "# Function to load a single feature from npz file\n",
    "def load_feature(id, folder_path, index):\n",
    "    file_path = os.path.join(folder_path, id)\n",
    "    data_f = np.load(file_path)\n",
    "    result = np.array(list(data_f.values()))\n",
    "    data_f.close()\n",
    "    return result, index\n",
    "# Function to load features in parallel and show progress bar\n",
    "def load_features_parallel(ids, folder_path):\n",
    "    \"\"\"Load features concurrently using ThreadPoolExecutor.\"\"\"\n",
    "    count = 0\n",
    "    id_num = len(ids)\n",
    "    features = [None] * id_num\n",
    "    with ThreadPoolExecutor(max_workers=128) as executor:\n",
    "        # Creating a list of futures\n",
    "        futures = [executor.submit(load_feature, str(file_id), folder_path, index) for index,file_id in enumerate(ids)]\n",
    "        # Progress bar for loading features\n",
    "        for future in as_completed(futures):\n",
    "            result, index = future.result()\n",
    "            count += 1\n",
    "            features[index] = result\n",
    "            if count % 50 == 0:\n",
    "                percent_complete = (count / id_num) * 100\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Progress: {percent_complete:.2f}% Complete ({count}/{id_num})\")\n",
    "\n",
    "    return np.array(features)\n",
    "\n",
    "# Folder path containing the feature files\n",
    "folder_path = \"datas\\\\IMDB_hids_no_extra_prompt\"\n",
    "\n",
    "# # Loading features for the training and test sets\n",
    "# X_train = load_features_parallel(train_df.index, folder_path)\n",
    "# X_test = load_features_parallel(test_df.index, folder_path)\n",
    "\n",
    "# # Labels for training and test sets\n",
    "# y_train = train_df['sentiment'].values\n",
    "# y_test = test_df['sentiment'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.py\n",
    "import numpy as np\n",
    "def process_file(file_id):\n",
    "    \"\"\"处理单个文件，返回合并后的array\"\"\"\n",
    "    file_path = f'datas\\\\IMDB_hids_no_extra_prompt\\\\{file_id}'  # 指定文件夹路径\n",
    "    data = np.load(file_path)\n",
    "    # 合并所有arrays为一个array\n",
    "    merged_array = np.array([data[f'arr_{i}'] for i in range(4096)])\n",
    "    data.close()\n",
    "    np.save(f'datas\\\\npdatas\\\\{file_id}.npy', merged_array)\n",
    "    return merged_array\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting loadone.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile loadone.py\n",
    "import numpy as np\n",
    "import os\n",
    "def load_feature(id_and_folder_path):\n",
    "    id, folder_path = id_and_folder_path\n",
    "    file_path = os.path.join(folder_path, str(id))\n",
    "    data_f = np.load(file_path)\n",
    "    result = np.array(list(data_f.values()))\n",
    "    data_f.close()\n",
    "    return result, id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [1:11:17<00:00, 11.69it/s]\n"
     ]
    }
   ],
   "source": [
    "import loadone\n",
    "from multiprocessing.pool import Pool\n",
    "from tqdm import tqdm  # 导入 tqdm\n",
    "import numpy as np\n",
    "\n",
    "folder_path = \"datas\\IMDB_hids_no_extra_prompt\"\n",
    "pool = Pool(30)\n",
    "task_num = 50000\n",
    "features = [None] * task_num\n",
    "arges = [(i, folder_path) for i in range(task_num)]\n",
    "# 使用 tqdm 包装 inputs，以显示进度条\n",
    "for result,id in tqdm(pool.imap_unordered(loadone.load_feature, arges), total=task_num):\n",
    "    features[id] = result\n",
    "\n",
    "pool.close()  # 关闭进程池，不再接受新的进程\n",
    "pool.join()   # 主进程阻塞等待子进程的退出\n",
    "np.save('datas\\\\50000.npy', np.array(features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "features_arr = np.load('datas\\\\50000.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 4096)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "file_path = 'datas\\\\IMDB_Dataset.csv'\n",
    "\n",
    "# 使用pandas读取CSV文件\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Splitting the DataFrame into training and test sets (80% training, 20% test)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['sentiment'], random_state=42)\n",
    "\n",
    "# Folder path containing the feature files\n",
    "folder_path = \"datas\\\\IMDB_hids_no_extra_prompt\"\n",
    "\n",
    "# # Loading features for the training and test sets\n",
    "X_train = features_arr[train_df.index]\n",
    "X_test = features_arr[test_df.index]\n",
    "\n",
    "# # Labels for training and test sets\n",
    "y_train = train_df['sentiment'].values\n",
    "y_test = test_df['sentiment'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_binary = np.array([1 if y == 'positive' else 0 for y in y_train])\n",
    "y_test_binary = np.array([1 if y == 'positive' else 0 for y in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 4096)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 支持向量机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM classifier\n",
    "svm_classifier = SVC()\n",
    "\n",
    "# Training the SVM classifier\n",
    "svm_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100\n",
      "building tree 2 of 100\n",
      "building tree 3 of 100\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n",
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  4.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n",
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.912\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.90      0.91      5000\n",
      "    positive       0.91      0.92      0.91      5000\n",
      "\n",
      "    accuracy                           0.91     10000\n",
      "   macro avg       0.91      0.91      0.91     10000\n",
      "weighted avg       0.91      0.91      0.91     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Create a Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42, verbose=2)\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 简单DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_DNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(4096, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 256)\n",
    "        self.fc3 = nn.Linear(256, 64)\n",
    "        self.fc4 = nn.Linear(64, 2)  # 假设是二分类问题\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'negative', 'negative', ..., 'negative', 'negative',\n",
       "       'negative'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设 X_train, y_train, X_test, y_test 已经是 NumPy 数组\n",
    "train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train_binary))\n",
    "test_dataset = TensorDataset(torch.FloatTensor(X_test), torch.LongTensor(y_test_binary))\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net_DNN()  # 确保你已经定义了 Net 类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Epoch 1/50, Loss: 0.2562997565627098, Accuracy: 0.8976\n",
      "Epoch 2/50, Loss: 0.18520880760550498, Accuracy: 0.93695\n",
      "Epoch 3/50, Loss: 0.17346595330238343, Accuracy: 0.941525\n",
      "Epoch 4/50, Loss: 0.17202396770119668, Accuracy: 0.9415\n",
      "Epoch 5/50, Loss: 0.16511010469794274, Accuracy: 0.944525\n",
      "Epoch 6/50, Loss: 0.16351269933879375, Accuracy: 0.9446\n",
      "Epoch 7/50, Loss: 0.16013710581362248, Accuracy: 0.9468\n",
      "Epoch 8/50, Loss: 0.15789192883968353, Accuracy: 0.94505\n",
      "Epoch 9/50, Loss: 0.15947627963721753, Accuracy: 0.94585\n",
      "Epoch 10/50, Loss: 0.1522055113583803, Accuracy: 0.947825\n",
      "Epoch 11/50, Loss: 0.15028146826922895, Accuracy: 0.947475\n",
      "Epoch 12/50, Loss: 0.15590590167939664, Accuracy: 0.9464\n",
      "Epoch 13/50, Loss: 0.14630998320877553, Accuracy: 0.9485\n",
      "Epoch 14/50, Loss: 0.14313860405683518, Accuracy: 0.9502\n",
      "Epoch 15/50, Loss: 0.14641722333729268, Accuracy: 0.949675\n",
      "Epoch 16/50, Loss: 0.1405715566277504, Accuracy: 0.951025\n",
      "Epoch 17/50, Loss: 0.138761558842659, Accuracy: 0.95125\n",
      "Epoch 18/50, Loss: 0.14515239610373973, Accuracy: 0.945775\n",
      "Epoch 19/50, Loss: 0.14159605745971202, Accuracy: 0.948425\n",
      "Epoch 20/50, Loss: 0.13832759206593037, Accuracy: 0.948375\n",
      "Epoch 21/50, Loss: 0.13963591074049472, Accuracy: 0.94945\n",
      "Epoch 22/50, Loss: 0.13262772646844387, Accuracy: 0.952325\n",
      "Epoch 23/50, Loss: 0.140596926510334, Accuracy: 0.946075\n",
      "Epoch 24/50, Loss: 0.13643550699055196, Accuracy: 0.947425\n",
      "Epoch 25/50, Loss: 0.13982761948108674, Accuracy: 0.949175\n",
      "Epoch 26/50, Loss: 0.1320192520737648, Accuracy: 0.9508\n",
      "Epoch 27/50, Loss: 0.1344315292865038, Accuracy: 0.948475\n",
      "Epoch 28/50, Loss: 0.1305389844983816, Accuracy: 0.94935\n",
      "Epoch 29/50, Loss: 0.13084684475362302, Accuracy: 0.95105\n",
      "Epoch 30/50, Loss: 0.13052336963117123, Accuracy: 0.951025\n",
      "Epoch 31/50, Loss: 0.12789824211150408, Accuracy: 0.95215\n",
      "Epoch 32/50, Loss: 0.12491885668635368, Accuracy: 0.953375\n",
      "Epoch 33/50, Loss: 0.1193090996593237, Accuracy: 0.955175\n",
      "Epoch 34/50, Loss: 0.1192653038546443, Accuracy: 0.952525\n",
      "Epoch 35/50, Loss: 0.11609780712127686, Accuracy: 0.9536\n",
      "Epoch 36/50, Loss: 0.120981157335639, Accuracy: 0.9524\n",
      "Epoch 37/50, Loss: 0.11765259465277195, Accuracy: 0.95645\n",
      "Epoch 38/50, Loss: 0.12144973604977131, Accuracy: 0.950725\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m correct_preds \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     24\u001b[0m total_preds \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 26\u001b[0m \u001b[39mfor\u001b[39;00m inputs, labels \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m     27\u001b[0m     \u001b[39m# 将数据也移动到相同的设备\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     inputs, labels \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     30\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\data_analyse\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\data_analyse\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\data_analyse\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\data_analyse\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\data_analyse\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\data_analyse\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\data_analyse\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[0;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\data_analyse\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[1;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 检查是否有可用的 GPU，如果有则使用 GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 定义模型\n",
    "\n",
    "\n",
    "# 将模型移至 GPU\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 50\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        # 将数据也移动到相同的设备\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_preds += (predicted == labels).sum().item()\n",
    "        total_preds += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = correct_preds / total_preds\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(epoch_acc)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}, Accuracy: {epoch_acc}')\n",
    "\n",
    "plt.plot(range(num_epochs), train_losses, label='Training Loss')\n",
    "plt.plot(range(num_epochs), train_accuracies, label='Training Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss/Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9441\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # 将数据移动到 GPU\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # 将预测和标签移回 CPU，并转换为 NumPy 数组\n",
    "        test_preds.extend(predicted.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 计算测试准确率\n",
    "accuracy = accuracy_score(test_labels, test_preds)\n",
    "print(f'Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 简单CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Net_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=8, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(16384*4, 512)  # 这里的2048取决于卷积和池化层后的特征维度\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 2)  # 假设是二分类问题\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # 增加一个通道维度\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  # 展平\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Epoch 1/50, Loss: 0.3562357508420944, Accuracy: 0.88775\n",
      "Epoch 2/50, Loss: 0.17190107627511025, Accuracy: 0.935225\n",
      "Epoch 3/50, Loss: 0.15371439773440362, Accuracy: 0.94275\n",
      "Epoch 4/50, Loss: 0.1383519442409277, Accuracy: 0.9487\n",
      "Epoch 5/50, Loss: 0.13238102878928185, Accuracy: 0.94985\n",
      "Epoch 6/50, Loss: 0.12078520303964616, Accuracy: 0.953375\n",
      "Epoch 7/50, Loss: 0.10907381809055805, Accuracy: 0.95895\n",
      "Epoch 8/50, Loss: 0.10292542172074318, Accuracy: 0.9612\n",
      "Epoch 9/50, Loss: 0.0996064413294196, Accuracy: 0.96225\n",
      "Epoch 10/50, Loss: 0.08987109827548266, Accuracy: 0.96585\n",
      "Epoch 11/50, Loss: 0.08665137266889214, Accuracy: 0.966425\n",
      "Epoch 12/50, Loss: 0.08196888773366809, Accuracy: 0.968375\n",
      "Epoch 13/50, Loss: 0.0770203395985067, Accuracy: 0.96885\n",
      "Epoch 14/50, Loss: 0.07474115806370973, Accuracy: 0.970425\n",
      "Epoch 15/50, Loss: 0.07252492879331111, Accuracy: 0.971825\n",
      "Epoch 16/50, Loss: 0.0689916726782918, Accuracy: 0.973475\n",
      "Epoch 17/50, Loss: 0.0667158260319382, Accuracy: 0.97335\n",
      "Epoch 18/50, Loss: 0.061285453770682216, Accuracy: 0.97565\n",
      "Epoch 19/50, Loss: 0.06207827007584274, Accuracy: 0.974825\n",
      "Epoch 20/50, Loss: 0.06138788907974958, Accuracy: 0.976275\n",
      "Epoch 21/50, Loss: 0.05846204368546605, Accuracy: 0.97625\n",
      "Epoch 22/50, Loss: 0.058930447766557335, Accuracy: 0.97655\n",
      "Epoch 23/50, Loss: 0.05466869031377137, Accuracy: 0.9781\n",
      "Epoch 24/50, Loss: 0.05445367812477052, Accuracy: 0.978125\n",
      "Epoch 25/50, Loss: 0.05492862828765065, Accuracy: 0.977775\n",
      "Epoch 26/50, Loss: 0.05315199097162113, Accuracy: 0.97835\n",
      "Epoch 27/50, Loss: 0.05304224360007793, Accuracy: 0.978975\n",
      "Epoch 28/50, Loss: 0.050160357037372885, Accuracy: 0.979625\n",
      "Epoch 29/50, Loss: 0.04860512440968305, Accuracy: 0.9802\n",
      "Epoch 30/50, Loss: 0.04897346441876143, Accuracy: 0.9805\n",
      "Epoch 31/50, Loss: 0.047681563973706216, Accuracy: 0.980425\n",
      "Epoch 32/50, Loss: 0.05089804740138352, Accuracy: 0.98\n",
      "Epoch 33/50, Loss: 0.04635727876015008, Accuracy: 0.98085\n",
      "Epoch 34/50, Loss: 0.0449587475977838, Accuracy: 0.9825\n",
      "Epoch 35/50, Loss: 0.04547764798775315, Accuracy: 0.981975\n",
      "Epoch 36/50, Loss: 0.04361060874406248, Accuracy: 0.9823\n",
      "Epoch 37/50, Loss: 0.042189423086494204, Accuracy: 0.98315\n",
      "Epoch 38/50, Loss: 0.04540321803065017, Accuracy: 0.9824\n",
      "Epoch 39/50, Loss: 0.04051967568639666, Accuracy: 0.98365\n",
      "Epoch 40/50, Loss: 0.042697279880207495, Accuracy: 0.982925\n",
      "Epoch 41/50, Loss: 0.0415482070366852, Accuracy: 0.98325\n",
      "Epoch 42/50, Loss: 0.040141775360982866, Accuracy: 0.983\n",
      "Epoch 43/50, Loss: 0.04109837849792093, Accuracy: 0.983025\n",
      "Epoch 44/50, Loss: 0.03926508650506148, Accuracy: 0.983875\n",
      "Epoch 45/50, Loss: 0.03769756116373464, Accuracy: 0.985075\n",
      "Epoch 46/50, Loss: 0.03942967711267993, Accuracy: 0.98455\n",
      "Epoch 47/50, Loss: 0.040093885889463124, Accuracy: 0.983675\n",
      "Epoch 48/50, Loss: 0.038506468502921054, Accuracy: 0.983925\n",
      "Epoch 49/50, Loss: 0.04016380331432447, Accuracy: 0.9836\n",
      "Epoch 50/50, Loss: 0.03895714010260999, Accuracy: 0.984425\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 假设 X_train, y_train 是您的训练数据和标签\n",
    "# 将它们转换为 PyTorch 的 Tensor\n",
    "train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train_binary))\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# 检查是否有可用的 GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 定义模型\n",
    "model = Net_CNN() # 使用您之前定义的 OneDCNN 类\n",
    "model = model.to(device)  # 将模型转移到 GPU\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 50  # 或者您选择的任何其他次数\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # 将数据转移到 GPU\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 计算预测的准确性\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_preds += (predicted == labels).sum().item()\n",
    "        total_preds += labels.size(0)\n",
    "\n",
    "    # 计算并打印每个 epoch 的平均损失和准确率\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = correct_preds / total_preds\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}, Accuracy: {epoch_acc}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.944\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # 将数据移动到 GPU\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # 将预测和标签移回 CPU，并转换为 NumPy 数组\n",
    "        test_preds.extend(predicted.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 计算测试准确率\n",
    "accuracy = accuracy_score(test_labels, test_preds)\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analyse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
